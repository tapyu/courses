{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some technical notes about audio parameters\n",
    "\n",
    "- The sampled signal is obtained in the Linear Pulse Code Modulation (LPCM).\n",
    "- The signal is stereo (`nchanells=2`), but it is only used the left-side signal.\n",
    "- It is utilized 16 bits (2 bytes) per sample to encode the audio. The native data type of this data is `int16`, which is capable of storing a [range from](https://www.mathworks.com/help/matlab/ref/audioread.html) `-32768` up to `+32767`.\n",
    "- The data type is converted to `float` because of the numeric precision and because the floating point in `Python` [is interpreted as](https://docs.python.org/3/library/stdtypes.html#numeric-types-int-float-complex) `double` in `C`, which is convenient.\n",
    "- The float-converted raw data is then normalized by the maximum value reachable of the `int16` format, that is, `32768`. The resulting signal is the same achieved by the `audioread()` command of `matlab`.\n",
    "- The original sampling rate is $44.1\\;kHz$. But each recording is downsampled into two different signals, with a sampling rate of $F_s = 22.05\\;kHz$.\n",
    "- The audio dataset comprises five classes (the speeches \"avançar\", \"recuar\", \"parar\", \"direita\", and \"esquerda\"), each with 10 recordings, totalizing 50 files. With the downsampling, we have 20 recordings by class. Considering that the `.wav` file is stereo, that is, `nchannel=2`, the number of audio recordings by class is increased to 40. From each of these recordings, it is extracted a discrete-time signal, which is converted to a $N_s$-dimensional vector, being $N_s$ the number of samples of this signal.\n",
    "\n",
    "## Some notes about the LPC (linear predictive coding) and the Yule-Walker algorithms\n",
    "\n",
    "- The AR(p) model is implemented for `p=10`, `p=15`, and `p=20`.\n",
    "- A single recording is divided into 31 frames without overlapping. The number of samples per frames, $N_f$, and the number of samples between each frame, $N_{gap}$, are given by\n",
    "    $$ N_f = \\frac{T_{sig}T_{f_{min}} F_s}{T_{min}} $$\n",
    "    and\n",
    "    $$ N_{gap} = \\frac{T_{sig} F_s-31N_f}{30},$$\n",
    "    where $T_{sig}$ is the signal duration, $T_{min}$ is the minimum signal duration of the dataset, and $T_{f_{min}} \\triangleq 15\\;ms $ is the minimum frame duration. All these variables are defined in seconds.\n",
    "- The Yule-Walker equation is applied to each of the 31 frames produced from a single audio recording. The final vector is achieved by concatenating all the $31$ coefficients obtained by the Yule-Walker equation, it is given by\n",
    "$$\\mathbf{a}_{p} = \\begin{bmatrix}\n",
    "\\mathbf{a}_{p,1}^\\mathsf{T} & \\mathbf{a}_{p,2}^\\mathsf{T} & \\cdots & \\mathbf{a}_{p,31}^\\mathsf{T}\n",
    "\\end{bmatrix}^\\mathsf{T} \\in \\mathbb{R}^{31p},\n",
    "$$\n",
    "where $\\mathbf{a}_{p,i} \\in \\mathbb{R}^p$ is the coefficient vector obtained form the $i$-th frame. This procedure is repeated for each of the four signals (channel a and b, samples even and odd) from a single audio recording, for each audio recording.\n",
    "- For sake of clarity, it  is chosen the normalized (by its l2 norm) version of the autocorrelation function. It makes $r(\\tau)$ invariant to the signal energy of the frame.\n",
    "- It is chosen a split for 50%-50% for train and train dataset, as done in the article.\n",
    "\n",
    "---\n",
    "\n",
    "> 1. Carregar os diversos arquivos de áudio e realizar a subamostragem dos sinais de cada canal a fim\n",
    "de gerar a base de dados de treino e teste.\n",
    "\n",
    "### Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reset -f\n",
    "from numpy import inf, empty, concatenate, arange, inner, array, logical_and, where, identity, logspace, zeros, any\n",
    "from numpy.linalg import norm, cond, matrix_rank as rank, inv\n",
    "from numpy.random import normal\n",
    "from numpy.fft import fft\n",
    "from statsmodels.regression.linear_model import yule_walker\n",
    "from scipy.io import wavfile\n",
    "from scipy.linalg import toeplitz\n",
    "from scipy.signal import periodogram\n",
    "from math import floor, log as ln, sqrt\n",
    "from warnings import warn\n",
    "from os import listdir\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# number of files per commands\n",
    "n_files = 10\n",
    "# AR(p) model order -> p = 10, 15, 20\n",
    "all_p = range(10,21,5)\n",
    "# all commands\n",
    "all_commands = ('avancar', 'esquerda', 'direita', 'parar', 'recuar')\n",
    "# a dictionary that gather all signals and the coefficient estimation for each audio file\n",
    "all_data = {f'{command}_{file_number}_p{p}_s{signal}': {'signal': empty(0), 'all_a': empty(0), 'all_a+noise_01': empty(0), 'all_a+noise_0625': empty(0), 'all_a_hat': empty(0), 'all_a_hat+noise_01': empty(0), 'all_a_hat+noise_0625': empty(0), 'all_a box-cox': empty(0), 'all_a+noise_01 box-cox': empty(0), 'all_a+noise_0625 box-cox': empty(0)} for p in all_p for command in all_commands for file_number in range(1,n_files+1) for signal in ('0a', '0b', '1a', '1b')}\n",
    "\n",
    "def get_T_min(root_dir):\n",
    "    T_min = inf\n",
    "    for file_name in listdir(root_dir):\n",
    "        F_s, s_n = wavfile.read(root_dir+file_name)\n",
    "        # signal duration\n",
    "        T_sig = s_n[:,0].size * (1/F_s)\n",
    "        if T_sig < T_min:\n",
    "            T_min = T_sig\n",
    "    return T_min\n",
    "\n",
    "def box_cox(x, gamma):\n",
    "    assert 0 <= gamma and gamma < 1, 'the hyperparameter should be 0<=gamma<1'\n",
    "    return ln(x) if gamma == 0 else (x**gamma -1)/gamma\n",
    "\n",
    "# minimum audio duration of the dataset\n",
    "T_min = get_T_min('./Audio_files_TCC_Jefferson/')\n",
    "# minimum frame duration, 15ms (user-defined)\n",
    "T_f_min = 15e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LPC and Yule-Walker algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reset_selective -f all_a\n",
    "for p in all_p:\n",
    "    for command in all_commands:\n",
    "        for file_number in range(1,n_files+1):\n",
    "            file_name = f'./Audio_files_TCC_Jefferson/comando_{command}_{file_number:0>2d}.wav'\n",
    "            # input audio vector, s_n -> [s[0], s[1], ..., s[N_s-1]]\n",
    "            F_s, s_n = wavfile.read(file_name)\n",
    "            # Number of samples\n",
    "            N_s = s_n[:,0].size\n",
    "            # signal duration\n",
    "            T_sig = N_s/F_s\n",
    "            # convert from int16 to float type and normalize it to range from -1 up to 1 (as matlab does)\n",
    "            s_n = s_n.astype(float)/32768\n",
    "            # downsampling: generate s0_n (even samples) and s1_n (odd samples) from s_n\n",
    "            s0_n, s1_n = s_n[range(0,N_s,2),:], s_n[range(1,N_s,2),:]\n",
    "            N_s //= 2\n",
    "            F_s //= 2\n",
    "            # number of samples per frame\n",
    "            N_f = floor(T_sig*T_f_min*F_s/T_min)\n",
    "            # number of samples between each frame (gap)\n",
    "            N_gap = floor((N_s - 31*N_f)/30)\n",
    "            # get channel a and chanell b\n",
    "            s0a_n, s0b_n, s1a_n, s1b_n = s0_n[:,0], s0_n[:,1], s1_n[:,0], s1_n[:,1]\n",
    "\n",
    "            # for each of the 4 signals from a single recording: channel a and b, samples even and odd\n",
    "            for s, sig_id in zip((s0a_n, s0b_n, s1a_n, s1b_n), ('0a', '0b', '1a', '1b')):\n",
    "                # save signal - no noise\n",
    "                all_data[f'{command}_{file_number}_p{p}_s{sig_id}']['signal'] = s\n",
    "                # set and save the one-hot-encoding\n",
    "                all_data[f'{command}_{file_number}_p{p}_s{sig_id}']['d'] = array([command==c for c in all_commands])\n",
    "                # generate and save signal + w ~ N(0,sd^2)\n",
    "                s_01 = s + sqrt(.01)*normal(size=s.shape)\n",
    "                s_0625 = s + sqrt(.0625)*normal(size=s.shape)\n",
    "                all_data[f'{command}_{file_number}_p{p}_s{sig_id}']['signal+noise_01'] = s_01\n",
    "                all_data[f'{command}_{file_number}_p{p}_s{sig_id}']['signal+noise_0625'] = s_0625\n",
    "                # for each noisy signal\n",
    "                for s_noise, noise_id in zip((s, s_01, s_0625), ('', '+noise_01', '+noise_0625')):\n",
    "                    # for each frame\n",
    "                    for i, n in enumerate(range(0, N_s+1, N_f)):\n",
    "                        # ensure that it is get only 31 frames\n",
    "                        if i == 31:\n",
    "                            break\n",
    "                        # s_n0 -> [s[n0], s[n0+1], ..., s[n0+N_f-1]], being n0\\in\\mathbb{N}\n",
    "                        s_n0 = s_noise[n+i*N_gap:n+i*N_gap+N_f]\n",
    "                        # compute the autocorrelation function (normalized version), r_k -> r[k] -> [r[0], r[1], ..., r[p]]\n",
    "                        r_k = empty(p+1)\n",
    "                        for k in range(p+1):\n",
    "                            r_k[k] = inner(s_n0[:N_f-k], s_n0[k:N_f])/norm(s_n0)\n",
    "                        # autocorrelation matrix\n",
    "                        # r_k[:p] -> [r[0], r[1], ..., r[p-1]]\n",
    "                        R = toeplitz(r_k[:p])\n",
    "                        # autocorrelation vector\n",
    "                        # r -> [r[1], r[2], ..., r[p]]\n",
    "                        r = r_k[1:]\n",
    "                        if rank(R) == R.shape[0]:\n",
    "                            # Yule-Walker equation\n",
    "                            a = inv(R) @ r\n",
    "                            # built-in function for comparison purpose\n",
    "                            a_hat, _ = yule_walker(s_n0, order=p, method='mle')\n",
    "                            # save a\n",
    "                            all_data[f'{command}_{file_number}_p{p}_s{sig_id}'][f'all_a{noise_id}'] = concatenate((all_data[f'{command}_{file_number}_p{p}_s{sig_id}'][f'all_a{noise_id}'], a))\n",
    "                            # save a_hat\n",
    "                            all_data[f'{command}_{file_number}_p{p}_s{sig_id}'][f'all_a_hat{noise_id}'] = concatenate((all_data[f'{command}_{file_number}_p{p}_s{sig_id}'][f'all_a_hat{noise_id}'], a_hat))\n",
    "                        else:\n",
    "                            warn(f'The autocorrelation matrix of the audio {file_name.split(\"/\")[0]} is rank-deficient, skip over to the next audio recording.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine each voice signal using the PSD segmentation method estimated by the periodogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my periodogram estimate function - FROM SCRATCH\n",
    "def my_periodogram(g, Fs):\n",
    "    N = g.size\n",
    "    # Nyquist theorem\n",
    "    W = Fs/2\n",
    "    # FFT\n",
    "    G = fft(g)\n",
    "    # two-sided periodogram estimate\n",
    "    p = abs(G)**2 / (N*Fs)\n",
    "    # transform it into one-sided\n",
    "    p = p[0:1+N//2]\n",
    "    p[1:-1] *= 2\n",
    "    # all frequencies\n",
    "    all_f = arange(0,W+Fs/N,Fs/N)[:p.size]\n",
    "    \n",
    "    return p, all_f\n",
    "\n",
    "# lower and upper bound\n",
    "all_lb = (0, 100, 200, 360, 500, 700, 850, 1000, 1200, 1320, 1500, 2200, 2700)\n",
    "all_ub = all_lb[1:] + (3700,)\n",
    "\n",
    "for key, data in all_data.items():\n",
    "    # As the 4 signals are identical for p \\in {10, 15, 20}, it is not necessary to compute x for different values of p\n",
    "    if 'p10' not in key:\n",
    "        continue\n",
    "    for noise_id in '', '+noise_01', '+noise_0625':\n",
    "        s = data[f'signal{noise_id}']\n",
    "        p, all_f = my_periodogram(s, F_s)\n",
    "        _, p_builtin = periodogram(s, fs=F_s)\n",
    "        # for each lower and upper bound interval, get the maximum value\n",
    "        psd_sampled = array([max(p[logical_and(all_f>=lb, all_f<=ub)]) for lb, ub in zip(all_lb, all_ub)])\n",
    "        # save the attribute vector and all PSD\n",
    "        all_data[key][f'psd{noise_id} sampled'] = psd_sampled\n",
    "        # save the attribute vector and all PSD with box-cox\n",
    "        all_data[key][f'psd{noise_id} sampled box-cox'] = box_cox(psd_sampled, 0.1)\n",
    "        # save PSD from my periodogram estimate\n",
    "        all_data[key][f'psd{noise_id}'] = p\n",
    "        # save PSD from my periodogram estimate - built-in function\n",
    "        all_data[key][f'psd{noise_id} built-in'] = p_builtin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Linear Least-Squares (LS) classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LS_Classifier():\n",
    "    def __init__(self):\n",
    "        self.n_errors = []\n",
    "        self.confusion_matrix_best = zeros((5, 5))\n",
    "        self.confusion_matrix_worst = zeros((5, 5))\n",
    "    \n",
    "    # training\n",
    "    def train(self, X, D):\n",
    "        # regularized estimation of the linear transformation matrix of the attribute vectors\n",
    "        for lambda_ in logspace(0,10,base=2):\n",
    "            W_hat = D @ X.T @ inv(X @ X.T + (lambda_-1)*identity(X.shape[0]))\n",
    "            # if W_hat is ill-conditioned, regularize it\n",
    "            if cond(X @ X.T + (lambda_-1)*identity(X.shape[0])) <= 5e3:\n",
    "                break\n",
    "        self.W_hat = W_hat\n",
    "    \n",
    "    # testing\n",
    "    def test(self, X, D):\n",
    "        n_errors = 0\n",
    "        confusion_matrix = zeros((5,5))\n",
    "        # testing phase\n",
    "        for x, d in zip(X.T, D.T): # for each column\n",
    "            d_hat = self.W_hat @ x\n",
    "            label = where(d == max(d))[0][0]\n",
    "            guessed = where(d_hat == max(d_hat))[0][0]\n",
    "            if label != guessed:\n",
    "                n_errors += 1\n",
    "            confusion_matrix[guessed][label] += 1\n",
    "        \n",
    "        # saving phase\n",
    "        if self.n_errors: # if self.n_errors is not empty\n",
    "            if n_errors > max(self.n_errors): # this is the worst test dataset?\n",
    "                self.confusion_matrix_worst = confusion_matrix\n",
    "            elif n_errors < min(self.n_errors): # this is the best test dataset?\n",
    "                self.confusion_matrix_best = confusion_matrix\n",
    "        else:\n",
    "            self.confusion_matrix_worst = confusion_matrix\n",
    "            self.confusion_matrix_best = confusion_matrix\n",
    "        # save n_errors\n",
    "        self.n_errors.append(n_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-fold cross validation: LS classifier + LPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26,\n",
       " 33,\n",
       " 32,\n",
       " 35,\n",
       " 32,\n",
       " 27,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 30,\n",
       " 23,\n",
       " 35,\n",
       " 39,\n",
       " 35,\n",
       " 32,\n",
       " 41,\n",
       " 34,\n",
       " 36,\n",
       " 38,\n",
       " 37,\n",
       " 36,\n",
       " 30,\n",
       " 29,\n",
       " 32,\n",
       " 27,\n",
       " 29,\n",
       " 32,\n",
       " 34,\n",
       " 32,\n",
       " 27,\n",
       " 31,\n",
       " 25,\n",
       " 31,\n",
       " 30,\n",
       " 32,\n",
       " 27,\n",
       " 34,\n",
       " 33,\n",
       " 32,\n",
       " 28,\n",
       " 32,\n",
       " 30,\n",
       " 31,\n",
       " 25,\n",
       " 29,\n",
       " 26,\n",
       " 39,\n",
       " 32,\n",
       " 28,\n",
       " 32,\n",
       " 31,\n",
       " 32,\n",
       " 28,\n",
       " 34,\n",
       " 28,\n",
       " 31,\n",
       " 29,\n",
       " 27,\n",
       " 31,\n",
       " 25,\n",
       " 26,\n",
       " 30,\n",
       " 31,\n",
       " 25,\n",
       " 24,\n",
       " 33,\n",
       " 29,\n",
       " 26,\n",
       " 31,\n",
       " 32,\n",
       " 30,\n",
       " 31,\n",
       " 36,\n",
       " 29,\n",
       " 28,\n",
       " 31,\n",
       " 25,\n",
       " 30,\n",
       " 31,\n",
       " 30,\n",
       " 27,\n",
       " 35,\n",
       " 29,\n",
       " 33,\n",
       " 32,\n",
       " 32,\n",
       " 24,\n",
       " 34,\n",
       " 37,\n",
       " 28,\n",
       " 34,\n",
       " 26,\n",
       " 32,\n",
       " 29,\n",
       " 25,\n",
       " 33,\n",
       " 24,\n",
       " 26,\n",
       " 27,\n",
       " 27,\n",
       " 30,\n",
       " 28,\n",
       " 25,\n",
       " 25,\n",
       " 32,\n",
       " 27,\n",
       " 29,\n",
       " 26,\n",
       " 28,\n",
       " 24,\n",
       " 31,\n",
       " 31,\n",
       " 28,\n",
       " 25,\n",
       " 29,\n",
       " 27,\n",
       " 30,\n",
       " 32,\n",
       " 26,\n",
       " 27,\n",
       " 33,\n",
       " 30,\n",
       " 26,\n",
       " 25,\n",
       " 30,\n",
       " 31,\n",
       " 33,\n",
       " 29,\n",
       " 33,\n",
       " 33,\n",
       " 22,\n",
       " 33,\n",
       " 33,\n",
       " 33,\n",
       " 29,\n",
       " 36,\n",
       " 33,\n",
       " 31,\n",
       " 37,\n",
       " 34,\n",
       " 32,\n",
       " 33,\n",
       " 30,\n",
       " 30,\n",
       " 25,\n",
       " 31,\n",
       " 26,\n",
       " 27,\n",
       " 31,\n",
       " 26,\n",
       " 25,\n",
       " 38,\n",
       " 30,\n",
       " 33,\n",
       " 32,\n",
       " 38,\n",
       " 34,\n",
       " 39,\n",
       " 37,\n",
       " 31,\n",
       " 40,\n",
       " 33,\n",
       " 35,\n",
       " 31,\n",
       " 28,\n",
       " 38,\n",
       " 33,\n",
       " 27,\n",
       " 30,\n",
       " 25,\n",
       " 25,\n",
       " 35,\n",
       " 29,\n",
       " 30,\n",
       " 30,\n",
       " 27,\n",
       " 27,\n",
       " 29,\n",
       " 35,\n",
       " 31,\n",
       " 25,\n",
       " 35,\n",
       " 31,\n",
       " 28,\n",
       " 31,\n",
       " 27,\n",
       " 27,\n",
       " 30,\n",
       " 25,\n",
       " 25,\n",
       " 22,\n",
       " 26,\n",
       " 30,\n",
       " 26,\n",
       " 23,\n",
       " 27,\n",
       " 34,\n",
       " 39,\n",
       " 38,\n",
       " 30,\n",
       " 33,\n",
       " 31,\n",
       " 30,\n",
       " 33,\n",
       " 28,\n",
       " 29,\n",
       " 32,\n",
       " 30,\n",
       " 31,\n",
       " 36,\n",
       " 30,\n",
       " 26,\n",
       " 36,\n",
       " 34,\n",
       " 30,\n",
       " 32,\n",
       " 37,\n",
       " 33,\n",
       " 32,\n",
       " 38,\n",
       " 34,\n",
       " 30,\n",
       " 33,\n",
       " 27,\n",
       " 29,\n",
       " 31,\n",
       " 28,\n",
       " 34,\n",
       " 27,\n",
       " 30,\n",
       " 32,\n",
       " 39,\n",
       " 32,\n",
       " 32,\n",
       " 38,\n",
       " 31,\n",
       " 29,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 29,\n",
       " 32,\n",
       " 35,\n",
       " 30,\n",
       " 29,\n",
       " 32,\n",
       " 36,\n",
       " 35,\n",
       " 32,\n",
       " 36,\n",
       " 30,\n",
       " 29]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LPC attribute vector size for LPC algorithm -> 31 frames x 10 attributes per frame\n",
    "P_LPC = 310\n",
    "n_folds = 10\n",
    "# columns_per_fold = X_LPC.shape[1]//10\n",
    "kfold_iterator = itertools.combinations(range(1,n_folds+1), n_folds//2)\n",
    "ls_classifier = LS_Classifier()\n",
    "\n",
    "for train_set in kfold_iterator:\n",
    "    # generate train dataset\n",
    "    X_train, D_train = empty((P_LPC, 0)), empty((5, 0))\n",
    "    all_data_train = {key:value for key, value in all_data.items() if 'p10' in key and any([key.split('_')[1] == str(e) for e in train_set])}\n",
    "    for data in all_data_train.values():\n",
    "        X_train = concatenate((X_train, data['all_a+noise_0625'][:,None]), axis=1)\n",
    "        D_train = concatenate((D_train, data['d'][:,None]), axis=1)\n",
    "    # generate test dataset\n",
    "    X_test, D_test = empty((P_LPC, 0)), empty((5, 0))\n",
    "    test_set = list(range(1,11))\n",
    "    for e in train_set:\n",
    "        test_set.remove(e)\n",
    "    all_data_test = {key:value for key, value in all_data.items() if 'p10' in key and any([key.split('_')[1] == str(e) for e in test_set])}\n",
    "    for data in all_data_test.values():\n",
    "        X_test = concatenate((X_test, data['all_a+noise_0625'][:,None]), axis=1)\n",
    "        D_test = concatenate((D_test, data['d'][:,None]), axis=1)\n",
    "    ls_classifier.train(X_train, D_train)\n",
    "    ls_classifier.test(X_test, D_test)\n",
    "\n",
    "ls_classifier.n_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-fold cross validation: LS classifier + PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[68,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 60,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 68,\n",
       " 76,\n",
       " 52,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 56,\n",
       " 64,\n",
       " 48,\n",
       " 52,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 60,\n",
       " 54,\n",
       " 64,\n",
       " 64,\n",
       " 60,\n",
       " 60,\n",
       " 64,\n",
       " 60,\n",
       " 64,\n",
       " 56,\n",
       " 56,\n",
       " 56,\n",
       " 64,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 64,\n",
       " 60,\n",
       " 64,\n",
       " 60,\n",
       " 60,\n",
       " 48,\n",
       " 60,\n",
       " 56,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 56,\n",
       " 56,\n",
       " 52,\n",
       " 68,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 60,\n",
       " 60,\n",
       " 68,\n",
       " 64,\n",
       " 60,\n",
       " 60,\n",
       " 64,\n",
       " 60,\n",
       " 64,\n",
       " 56,\n",
       " 48,\n",
       " 60,\n",
       " 68,\n",
       " 64,\n",
       " 60,\n",
       " 60,\n",
       " 64,\n",
       " 56,\n",
       " 64,\n",
       " 52,\n",
       " 56,\n",
       " 56,\n",
       " 60,\n",
       " 60,\n",
       " 64,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 52,\n",
       " 56,\n",
       " 48,\n",
       " 60,\n",
       " 68,\n",
       " 64,\n",
       " 60,\n",
       " 60,\n",
       " 64,\n",
       " 60,\n",
       " 64,\n",
       " 56,\n",
       " 52,\n",
       " 60,\n",
       " 60,\n",
       " 56,\n",
       " 64,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 48,\n",
       " 56,\n",
       " 60,\n",
       " 60,\n",
       " 64,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 52,\n",
       " 56,\n",
       " 48,\n",
       " 52,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 56,\n",
       " 72,\n",
       " 64,\n",
       " 64,\n",
       " 70,\n",
       " 80,\n",
       " 68,\n",
       " 68,\n",
       " 64,\n",
       " 60,\n",
       " 64,\n",
       " 64,\n",
       " 60,\n",
       " 64,\n",
       " 58,\n",
       " 52,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 60,\n",
       " 64,\n",
       " 64,\n",
       " 56,\n",
       " 68,\n",
       " 68,\n",
       " 76,\n",
       " 60,\n",
       " 56,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 56,\n",
       " 52,\n",
       " 56,\n",
       " 52,\n",
       " 68,\n",
       " 68,\n",
       " 64,\n",
       " 60,\n",
       " 64,\n",
       " 64,\n",
       " 60,\n",
       " 64,\n",
       " 60,\n",
       " 64,\n",
       " 60,\n",
       " 64,\n",
       " 60,\n",
       " 64,\n",
       " 60,\n",
       " 60,\n",
       " 64,\n",
       " 60,\n",
       " 60,\n",
       " 52,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 64,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 52,\n",
       " 56,\n",
       " 52,\n",
       " 56,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 56,\n",
       " 68,\n",
       " 68,\n",
       " 64,\n",
       " 60,\n",
       " 64,\n",
       " 64,\n",
       " 60,\n",
       " 64,\n",
       " 56,\n",
       " 60,\n",
       " 68,\n",
       " 60,\n",
       " 60,\n",
       " 64,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 43,\n",
       " 44,\n",
       " 64,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 52,\n",
       " 52,\n",
       " 48,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 48,\n",
       " 68,\n",
       " 64,\n",
       " 60,\n",
       " 64,\n",
       " 60,\n",
       " 60,\n",
       " 64,\n",
       " 60,\n",
       " 56,\n",
       " 44,\n",
       " 60,\n",
       " 56,\n",
       " 56,\n",
       " 60,\n",
       " 48,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 48,\n",
       " 60]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PSD attribute vector size -> 13 frames of the PSD\n",
    "P_PSD = 13\n",
    "# columns_per_fold = X_LPC.shape[1]//10\n",
    "kfold_iterator = itertools.combinations(range(1,n_folds+1), n_folds//2)\n",
    "ls_classifier = LS_Classifier()\n",
    "\n",
    "for train_set in kfold_iterator:\n",
    "    # generate train dataset\n",
    "    X_train, D_train = empty((P_PSD, 0)), empty((5, 0))\n",
    "    all_data_train = {key:value for key, value in all_data.items() if 'p10' in key and any([key.split('_')[1] == str(e) for e in train_set])}\n",
    "    for data in all_data_train.values():\n",
    "        X_train = concatenate((X_train, data['psd sampled'][:,None]), axis=1)\n",
    "        D_train = concatenate((D_train, data['d'][:,None]), axis=1)\n",
    "    # generate test dataset\n",
    "    X_test, D_test = empty((P_PSD, 0)), empty((5, 0))\n",
    "    test_set = list(range(1,11))\n",
    "    for e in train_set:\n",
    "        test_set.remove(e)\n",
    "    all_data_test = {key:value for key, value in all_data.items() if 'p10' in key and any([key.split('_')[1] == str(e) for e in test_set])}\n",
    "    for data in all_data_test.values():\n",
    "        X_test = concatenate((X_test, data['psd sampled'][:,None]), axis=1)\n",
    "        D_test = concatenate((D_test, data['d'][:,None]), axis=1)\n",
    "    ls_classifier.train(X_train, D_train)\n",
    "    ls_classifier.test(X_test, D_test)\n",
    "\n",
    "ls_classifier.n_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold cross validation: LS classifier + PSD + Box-Cox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42,\n",
       " 33,\n",
       " 33,\n",
       " 38,\n",
       " 44,\n",
       " 46,\n",
       " 40,\n",
       " 41,\n",
       " 43,\n",
       " 39,\n",
       " 40,\n",
       " 35,\n",
       " 41,\n",
       " 38,\n",
       " 33,\n",
       " 34,\n",
       " 41,\n",
       " 37,\n",
       " 45,\n",
       " 39,\n",
       " 52,\n",
       " 31,\n",
       " 33,\n",
       " 34,\n",
       " 41,\n",
       " 38,\n",
       " 24,\n",
       " 32,\n",
       " 37,\n",
       " 32,\n",
       " 32,\n",
       " 39,\n",
       " 39,\n",
       " 46,\n",
       " 41,\n",
       " 55,\n",
       " 29,\n",
       " 38,\n",
       " 34,\n",
       " 28,\n",
       " 32,\n",
       " 34,\n",
       " 34,\n",
       " 37,\n",
       " 38,\n",
       " 48,\n",
       " 33,\n",
       " 34,\n",
       " 34,\n",
       " 37,\n",
       " 35,\n",
       " 41,\n",
       " 40,\n",
       " 38,\n",
       " 48,\n",
       " 53,\n",
       " 29,\n",
       " 31,\n",
       " 37,\n",
       " 40,\n",
       " 39,\n",
       " 24,\n",
       " 29,\n",
       " 35,\n",
       " 30,\n",
       " 32,\n",
       " 40,\n",
       " 34,\n",
       " 47,\n",
       " 40,\n",
       " 53,\n",
       " 25,\n",
       " 32,\n",
       " 30,\n",
       " 23,\n",
       " 27,\n",
       " 30,\n",
       " 29,\n",
       " 36,\n",
       " 36,\n",
       " 41,\n",
       " 30,\n",
       " 31,\n",
       " 29,\n",
       " 33,\n",
       " 32,\n",
       " 39,\n",
       " 37,\n",
       " 36,\n",
       " 47,\n",
       " 51,\n",
       " 27,\n",
       " 35,\n",
       " 35,\n",
       " 34,\n",
       " 36,\n",
       " 43,\n",
       " 39,\n",
       " 49,\n",
       " 44,\n",
       " 52,\n",
       " 32,\n",
       " 36,\n",
       " 34,\n",
       " 43,\n",
       " 39,\n",
       " 48,\n",
       " 47,\n",
       " 45,\n",
       " 54,\n",
       " 58,\n",
       " 28,\n",
       " 30,\n",
       " 31,\n",
       " 33,\n",
       " 32,\n",
       " 38,\n",
       " 36,\n",
       " 36,\n",
       " 44,\n",
       " 48,\n",
       " 41,\n",
       " 41,\n",
       " 42,\n",
       " 48,\n",
       " 52,\n",
       " 22,\n",
       " 35,\n",
       " 33,\n",
       " 40,\n",
       " 41,\n",
       " 17,\n",
       " 23,\n",
       " 27,\n",
       " 25,\n",
       " 27,\n",
       " 32,\n",
       " 33,\n",
       " 36,\n",
       " 39,\n",
       " 55,\n",
       " 22,\n",
       " 19,\n",
       " 27,\n",
       " 20,\n",
       " 20,\n",
       " 28,\n",
       " 32,\n",
       " 30,\n",
       " 32,\n",
       " 42,\n",
       " 30,\n",
       " 29,\n",
       " 28,\n",
       " 31,\n",
       " 31,\n",
       " 36,\n",
       " 38,\n",
       " 37,\n",
       " 45,\n",
       " 48,\n",
       " 16,\n",
       " 25,\n",
       " 26,\n",
       " 26,\n",
       " 32,\n",
       " 34,\n",
       " 39,\n",
       " 37,\n",
       " 41,\n",
       " 53,\n",
       " 31,\n",
       " 33,\n",
       " 32,\n",
       " 37,\n",
       " 31,\n",
       " 42,\n",
       " 42,\n",
       " 38,\n",
       " 49,\n",
       " 54,\n",
       " 29,\n",
       " 27,\n",
       " 26,\n",
       " 29,\n",
       " 30,\n",
       " 32,\n",
       " 36,\n",
       " 39,\n",
       " 43,\n",
       " 45,\n",
       " 39,\n",
       " 38,\n",
       " 39,\n",
       " 42,\n",
       " 50,\n",
       " 15,\n",
       " 20,\n",
       " 24,\n",
       " 24,\n",
       " 34,\n",
       " 39,\n",
       " 37,\n",
       " 38,\n",
       " 37,\n",
       " 50,\n",
       " 33,\n",
       " 32,\n",
       " 33,\n",
       " 39,\n",
       " 36,\n",
       " 44,\n",
       " 44,\n",
       " 44,\n",
       " 48,\n",
       " 56,\n",
       " 27,\n",
       " 22,\n",
       " 22,\n",
       " 27,\n",
       " 26,\n",
       " 27,\n",
       " 35,\n",
       " 34,\n",
       " 37,\n",
       " 41,\n",
       " 40,\n",
       " 39,\n",
       " 37,\n",
       " 41,\n",
       " 50,\n",
       " 35,\n",
       " 40,\n",
       " 35,\n",
       " 38,\n",
       " 38,\n",
       " 43,\n",
       " 48,\n",
       " 43,\n",
       " 53,\n",
       " 54,\n",
       " 48,\n",
       " 48,\n",
       " 48,\n",
       " 51,\n",
       " 56,\n",
       " 42,\n",
       " 39,\n",
       " 39,\n",
       " 44,\n",
       " 48,\n",
       " 51]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns_per_fold = X_LPC.shape[1]//10\n",
    "kfold_iterator = itertools.combinations(range(1,n_folds+1), n_folds//2)\n",
    "ls_classifier = LS_Classifier()\n",
    "\n",
    "for train_set in kfold_iterator:\n",
    "    # generate train dataset\n",
    "    X_train, D_train = empty((P_PSD, 0)), empty((5, 0))\n",
    "    all_data_train = {key:value for key, value in all_data.items() if 'p10' in key and any([key.split('_')[1] == str(e) for e in train_set])}\n",
    "    for data in all_data_train.values():\n",
    "        X_train = concatenate((X_train, data['psd sampled box-cox'][:,None]), axis=1)\n",
    "        D_train = concatenate((D_train, data['d'][:,None]), axis=1)\n",
    "    # generate test dataset\n",
    "    X_test, D_test = empty((P_PSD, 0)), empty((5, 0))\n",
    "    test_set = list(range(1,11))\n",
    "    for e in train_set:\n",
    "        test_set.remove(e)\n",
    "    all_data_test = {key:value for key, value in all_data.items() if 'p10' in key and any([key.split('_')[1] == str(e) for e in test_set])}\n",
    "    for data in all_data_test.values():\n",
    "        X_test = concatenate((X_test, data['psd sampled box-cox'][:,None]), axis=1)\n",
    "        D_test = concatenate((D_test, data['d'][:,None]), axis=1)\n",
    "    ls_classifier.train(X_train, D_train)\n",
    "    ls_classifier.test(X_test, D_test)\n",
    "\n",
    "ls_classifier.n_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.,  0.,  0.,  0.,  8.],\n",
       "       [ 0.,  8.,  4.,  0.,  4.],\n",
       "       [ 0., 12., 16.,  0.,  0.],\n",
       "       [16.,  0.,  0., 20.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  8.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_classifier.confusion_matrix_worst"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d7aca15fd486567410f0b50ac6cefe479aeb320e6005d7aafae29a8f55f6329"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('identificação-de-sistemas-sFYUFp5A-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
