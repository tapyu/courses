{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some technical notes about audio parameters\n",
    "\n",
    "- The sampled signal is obtained in the Linear Pulse Code Modulation (LPCM).\n",
    "- The signal is stereo (`nchanells=2`), but it is only used the left-side signal.\n",
    "- It is utilized 16 bits (2 bytes) per sample to encode the audio. The native data type of this data is `int16`, which is capable of storing a [range from](https://www.mathworks.com/help/matlab/ref/audioread.html) `-32768` up to `+32767`.\n",
    "- The data type is converted to `float` because of the numeric precision and because the floating point in `Python` [is interpreted as](https://docs.python.org/3/library/stdtypes.html#numeric-types-int-float-complex) `double` in `C`, which is convenient.\n",
    "- The raw data is normalized by its l2 norm for each frame.\n",
    "- The original sampling rate is $44.1\\;kHz$. But each recording is downsampled into two different signals, with a sampling rate of $F_s = 22.05\\;kHz$.\n",
    "- The audio dataset comprises five classes (the speeches \"avançar\", \"recuar\", \"parar\", \"direita\", and \"esquerda\"), each with 10 recordings, totalizing 50 files. With the downsampling, we have 20 recordings by class. Considering that the `.wav` file is stereo, that is, `nchannel=2`, the number of audio recordings by class is increased to 40. From each of these recordings, it is extracted a discrete-time signal, which is converted to a $N_s$-dimensional vector, being $N_s$ the number of samples of this signal.\n",
    "\n",
    "## Some notes about the LPC (linear predictive coding) and the Yule-Walker algorithms\n",
    "\n",
    "- The AR(p) model is implemented for `p=10`, `p=15`, and `p=20`.\n",
    "- A single recording is divided into 31 frames without overlapping. The number of samples per frames, $N_f$, and the number of samples between each frame, $N_{gap}$, are given by\n",
    "    $$ N_f = \\frac{T_{sig}T_{f_{min}} F_s}{T_{min}} $$\n",
    "    and\n",
    "    $$ N_{gap} = \\frac{T_{sig} F_s-31N_f}{30},$$\n",
    "    where $T_{sig}$ is the signal duration, $T_{min}$ is the minimum signal duration of the dataset, and $T_{f_{min}} \\triangleq 15\\;ms $ is the minimum frame duration. All these variables are defined in seconds.\n",
    "<!-- - The Yule-Walker equation is applied to each of the 31 frames produced from a single audio recording. Being $\\mathbf{a}_{i,j} \\in \\mathbb{R}^{p}$ the $j$-th vector of the $i$-th audio recording, the matrix containing all coefficients of the AR(p) of the $i$-th audio recording is\n",
    "$$\\mathbf{A}_i = \\begin{bmatrix}\n",
    "\\mathbf{a}_{i,1} & \\mathbf{a}_{i,2} & \\cdots & \\mathbf{a}_{i,31}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{p\\times 31}\n",
    "$$ -->\n",
    "\n",
    "---\n",
    "\n",
    "> 1. Carregar os diversos arquivos de áudio e realizar a subamostragem dos sinais de cada canal a fim\n",
    "de gerar a base de dados de treino e teste.\n",
    "\n",
    "### Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import multiply, sum, matmul, inf, empty, concatenate, zeros, array\n",
    "from statsmodels.regression.linear_model import yule_walker\n",
    "from scipy.io import wavfile\n",
    "from scipy.linalg import toeplitz\n",
    "from math import floor\n",
    "from numpy.linalg import norm, cond, matrix_rank as rank, inv\n",
    "from warnings import warn\n",
    "from os import listdir\n",
    "\n",
    "# train/test set split\n",
    "n_train, n_test = 8, 2\n",
    "# AR(p) model order -> p = 10, 15, 20\n",
    "all_p = range(10,21,5)\n",
    "# all coefficients of the AR(p) model. For each command, we have a 8 set of coefficients\n",
    "all_a = {f'{command}_file{file_number}_p{p}_s{signal}': array([]) for p in all_p for command in ('avancar', 'esquerda', 'direita', 'parar', 'recuar') for file_number in range(1,11) for signal in ('1a', '1b', '2a', '2b')}\n",
    "\n",
    "def get_T_min(root_dir):\n",
    "    T_min = inf\n",
    "    for file_name in listdir(root_dir):\n",
    "        F_s, s_n = wavfile.read(root_dir+file_name)\n",
    "        # signal duration\n",
    "        T_sig = s_n[:,0].size * (1/F_s)\n",
    "        if T_sig < T_min:\n",
    "            T_min = T_sig\n",
    "    return T_min\n",
    "\n",
    "# minimum audio duration of the dataset\n",
    "T_min = get_T_min('./Audio_files_TCC_Jefferson/')\n",
    "# minimum frame duration, 15ms (user defined)\n",
    "T_f_min = 15e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LPC and Yule-Walker algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_77827/1175851746.py:49: UserWarning: The autocorrelation matrix of the audio ./Audio_files_TCC_Jefferson/comando_avancar_01.wav is ill-conditioned! The results are suspect!\n",
      "  warn(f'The autocorrelation matrix of the audio {file_name} is ill-conditioned! The results are suspect!')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/tapyu/git/UFC_courses/identificação_de_sistemas/Projeto 1 - Extração de Atributos de Sinais de Voz para Classificação de Padrões/TC1.ipynb Cell 4'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tapyu/git/UFC_courses/identifica%C3%A7%C3%A3o_de_sistemas/Projeto%201%20-%20Extra%C3%A7%C3%A3o%20de%20Atributos%20de%20Sinais%20de%20Voz%20para%20Classifica%C3%A7%C3%A3o%20de%20Padr%C3%B5es/TC1.ipynb#ch0000003?line=51'>52</a>\u001b[0m     \u001b[39m# built-in function for comparasin purpose\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tapyu/git/UFC_courses/identifica%C3%A7%C3%A3o_de_sistemas/Projeto%201%20-%20Extra%C3%A7%C3%A3o%20de%20Atributos%20de%20Sinais%20de%20Voz%20para%20Classifica%C3%A7%C3%A3o%20de%20Padr%C3%B5es/TC1.ipynb#ch0000003?line=52'>53</a>\u001b[0m     a_hat, _ \u001b[39m=\u001b[39m yule_walker(s_n0, order\u001b[39m=\u001b[39mp)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tapyu/git/UFC_courses/identifica%C3%A7%C3%A3o_de_sistemas/Projeto%201%20-%20Extra%C3%A7%C3%A3o%20de%20Atributos%20de%20Sinais%20de%20Voz%20para%20Classifica%C3%A7%C3%A3o%20de%20Padr%C3%B5es/TC1.ipynb#ch0000003?line=53'>54</a>\u001b[0m     all_a[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcommand\u001b[39m}\u001b[39;00m\u001b[39m_file\u001b[39m\u001b[39m{\u001b[39;00mfile_number\u001b[39m}\u001b[39;00m\u001b[39m_p\u001b[39m\u001b[39m{\u001b[39;00mp\u001b[39m}\u001b[39;00m\u001b[39m_s\u001b[39m\u001b[39m{\u001b[39;00msignal_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m concatenate((all_a[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcommand\u001b[39m}\u001b[39;00m\u001b[39m_file\u001b[39m\u001b[39m{\u001b[39;00mfile_number\u001b[39m}\u001b[39;00m\u001b[39m_p\u001b[39m\u001b[39m{\u001b[39;00mp\u001b[39m}\u001b[39;00m\u001b[39m_s\u001b[39m\u001b[39m{\u001b[39;00msignal_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m], a))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tapyu/git/UFC_courses/identifica%C3%A7%C3%A3o_de_sistemas/Projeto%201%20-%20Extra%C3%A7%C3%A3o%20de%20Atributos%20de%20Sinais%20de%20Voz%20para%20Classifica%C3%A7%C3%A3o%20de%20Padr%C3%B5es/TC1.ipynb#ch0000003?line=54'>55</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tapyu/git/UFC_courses/identifica%C3%A7%C3%A3o_de_sistemas/Projeto%201%20-%20Extra%C3%A7%C3%A3o%20de%20Atributos%20de%20Sinais%20de%20Voz%20para%20Classifica%C3%A7%C3%A3o%20de%20Padr%C3%B5es/TC1.ipynb#ch0000003?line=55'>56</a>\u001b[0m     warn(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThe autocorrelation matrix of the audio \u001b[39m\u001b[39m{\u001b[39;00mfile_name\u001b[39m}\u001b[39;00m\u001b[39m is rank-deficient, skip over to the next audio recording.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/tapyu/git/UFC_courses/identificação_de_sistemas/Projeto 1 - Extração de Atributos de Sinais de Voz para Classificação de Padrões/TC1.ipynb Cell 4'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tapyu/git/UFC_courses/identifica%C3%A7%C3%A3o_de_sistemas/Projeto%201%20-%20Extra%C3%A7%C3%A3o%20de%20Atributos%20de%20Sinais%20de%20Voz%20para%20Classifica%C3%A7%C3%A3o%20de%20Padr%C3%B5es/TC1.ipynb#ch0000003?line=51'>52</a>\u001b[0m     \u001b[39m# built-in function for comparasin purpose\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tapyu/git/UFC_courses/identifica%C3%A7%C3%A3o_de_sistemas/Projeto%201%20-%20Extra%C3%A7%C3%A3o%20de%20Atributos%20de%20Sinais%20de%20Voz%20para%20Classifica%C3%A7%C3%A3o%20de%20Padr%C3%B5es/TC1.ipynb#ch0000003?line=52'>53</a>\u001b[0m     a_hat, _ \u001b[39m=\u001b[39m yule_walker(s_n0, order\u001b[39m=\u001b[39mp)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tapyu/git/UFC_courses/identifica%C3%A7%C3%A3o_de_sistemas/Projeto%201%20-%20Extra%C3%A7%C3%A3o%20de%20Atributos%20de%20Sinais%20de%20Voz%20para%20Classifica%C3%A7%C3%A3o%20de%20Padr%C3%B5es/TC1.ipynb#ch0000003?line=53'>54</a>\u001b[0m     all_a[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcommand\u001b[39m}\u001b[39;00m\u001b[39m_file\u001b[39m\u001b[39m{\u001b[39;00mfile_number\u001b[39m}\u001b[39;00m\u001b[39m_p\u001b[39m\u001b[39m{\u001b[39;00mp\u001b[39m}\u001b[39;00m\u001b[39m_s\u001b[39m\u001b[39m{\u001b[39;00msignal_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m concatenate((all_a[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcommand\u001b[39m}\u001b[39;00m\u001b[39m_file\u001b[39m\u001b[39m{\u001b[39;00mfile_number\u001b[39m}\u001b[39;00m\u001b[39m_p\u001b[39m\u001b[39m{\u001b[39;00mp\u001b[39m}\u001b[39;00m\u001b[39m_s\u001b[39m\u001b[39m{\u001b[39;00msignal_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m], a))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tapyu/git/UFC_courses/identifica%C3%A7%C3%A3o_de_sistemas/Projeto%201%20-%20Extra%C3%A7%C3%A3o%20de%20Atributos%20de%20Sinais%20de%20Voz%20para%20Classifica%C3%A7%C3%A3o%20de%20Padr%C3%B5es/TC1.ipynb#ch0000003?line=54'>55</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tapyu/git/UFC_courses/identifica%C3%A7%C3%A3o_de_sistemas/Projeto%201%20-%20Extra%C3%A7%C3%A3o%20de%20Atributos%20de%20Sinais%20de%20Voz%20para%20Classifica%C3%A7%C3%A3o%20de%20Padr%C3%B5es/TC1.ipynb#ch0000003?line=55'>56</a>\u001b[0m     warn(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThe autocorrelation matrix of the audio \u001b[39m\u001b[39m{\u001b[39;00mfile_name\u001b[39m}\u001b[39;00m\u001b[39m is rank-deficient, skip over to the next audio recording.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1363\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:662\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1288\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1250\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:297\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/id-sist-sFYUFp5A-py3.8/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py:1976\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.cache/pypoetry/virtualenvs/id-sist-sFYUFp5A-py3.8/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py?line=1972'>1973</a>\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   <a href='file:///~/.cache/pypoetry/virtualenvs/id-sist-sFYUFp5A-py3.8/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py?line=1974'>1975</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[0;32m-> <a href='file:///~/.cache/pypoetry/virtualenvs/id-sist-sFYUFp5A-py3.8/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py?line=1975'>1976</a>\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   <a href='file:///~/.cache/pypoetry/virtualenvs/id-sist-sFYUFp5A-py3.8/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py?line=1977'>1978</a>\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.cache/pypoetry/virtualenvs/id-sist-sFYUFp5A-py3.8/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py?line=1979'>1980</a>\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   <a href='file:///~/.cache/pypoetry/virtualenvs/id-sist-sFYUFp5A-py3.8/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py?line=1980'>1981</a>\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/id-sist-sFYUFp5A-py3.8/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py:2011\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.cache/pypoetry/virtualenvs/id-sist-sFYUFp5A-py3.8/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py?line=2007'>2008</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_mpl_hook()\n\u001b[1;32m   <a href='file:///~/.cache/pypoetry/virtualenvs/id-sist-sFYUFp5A-py3.8/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py?line=2009'>2010</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> <a href='file:///~/.cache/pypoetry/virtualenvs/id-sist-sFYUFp5A-py3.8/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py?line=2010'>2011</a>\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   <a href='file:///~/.cache/pypoetry/virtualenvs/id-sist-sFYUFp5A-py3.8/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py?line=2012'>2013</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   <a href='file:///~/.cache/pypoetry/virtualenvs/id-sist-sFYUFp5A-py3.8/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py?line=2014'>2015</a>\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for p in all_p:\n",
    "    for command in ('avancar', 'direita', 'esquerda', 'parar', 'recuar'):\n",
    "        # training set\n",
    "        for file_number in range(1,n_train+1):\n",
    "            file_name = f'./Audio_files_TCC_Jefferson/comando_{command}_{file_number:0>2d}.wav'\n",
    "            # input audio vector, s_n -> [s[0], s[1], ..., s[N_s-1]]\n",
    "            F_s, s_n = wavfile.read(file_name)\n",
    "            # Number of samples\n",
    "            N_s = s_n[:,0].size\n",
    "            # signal duration\n",
    "            T_sig = N_s/F_s\n",
    "            # convert from int16 to float type\n",
    "            s_n = s_n.astype(float)\n",
    "            # downsampling: generate s0_n (even samples) and s1_n (odd samples) from s_n\n",
    "            s0_n, s1_n = s_n[range(0,N_s,2),:], s_n[range(1,N_s,2),:]\n",
    "            N_s //= 2\n",
    "            F_s /= 2\n",
    "            # number of samples per frame\n",
    "            N_f = floor(T_sig*T_f_min*F_s/T_min)\n",
    "            # number of samples between each frame (gap)\n",
    "            N_gap = floor((N_s - 31*N_f)/30)\n",
    "            # get channel b and chanell b\n",
    "            s0a_n, s0b_n, s1a_n, s1b_n = s0_n[:,0], s0_n[:,1], s1_n[:,0], s1_n[:,1]\n",
    "\n",
    "            # for each of the 4 signals from a single recording: channel a and b, samples even and odd\n",
    "            for s, signal_id in zip((s0a_n, s0b_n, s1a_n, s1b_n), ('1a', '1b', '2a', '2b')):\n",
    "                # for each frame\n",
    "                for i, n in enumerate(range(0, N_s+1, N_f)):\n",
    "                    # ensure that it is get only 31 frames\n",
    "                    if i == 31: break\n",
    "                    # s_n0 -> [s[n0], s[n0+1], ..., s[n0+N_f-1]], being n0\\in\\mathbb{N}\n",
    "                    s_n0 = s[n+i*N_gap:n+i*N_gap+N_f]\n",
    "                    # normalized signal by its l2 norm\n",
    "                    s_n0 /= norm(s_n0)\n",
    "                    # compute the autocorrelation function, r_k -> r[k] -> [r[0], r[1], ..., r[p]]\n",
    "                    r_k = empty(p+1)\n",
    "                    for k in range(p+1):\n",
    "                        # s_n0_minus_k -> [0, 0, ..., 0(k times), s[n0], s[n0+1], ..., s[n0+N_f-1-k]]\n",
    "                        s_n0_minus_k = concatenate((zeros(k), s_n0[k:]))\n",
    "                        r_k[k] = sum(multiply(s_n0, s_n0_minus_k))\n",
    "                    # autocorrelation matrix\n",
    "                    # r_k[:p] -> [r[0], r[1], ..., r[p-1]]\n",
    "                    R = toeplitz(r_k[:p])\n",
    "                    # autocorrelation vector\n",
    "                    # r -> [r[1], r[2], ..., r[p]]\n",
    "                    r = r_k[1:]\n",
    "                    if rank(R) == R.shape[0]:\n",
    "                        if cond(R) > 1e3:\n",
    "                            warn(f'The autocorrelation matrix of the audio {file_name} is ill-conditioned! The results are suspect!')\n",
    "                        # Yule-Walker equation\n",
    "                        a = matmul(inv(R), r)\n",
    "                        # built-in function for comparasin purpose\n",
    "                        a_hat, _ = yule_walker(s_n0, order=p)\n",
    "                        all_a[f'{command}_file{file_number}_p{p}_s{signal_id}'] = concatenate((all_a[f'{command}_file{file_number}_p{p}_s{signal_id}'], a))\n",
    "                    else:\n",
    "                        warn(f'The autocorrelation matrix of the audio {file_name} is rank-deficient, skip over to the next audio recording.')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d7aca15fd486567410f0b50ac6cefe479aeb320e6005d7aafae29a8f55f6329"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('identificação-de-sistemas-sFYUFp5A-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
