{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some technical notes about audio parameters\n",
    "\n",
    "- The sampled signal is obtained in the Linear Pulse Code Modulation (LPCM).\n",
    "- The signal is stereo (`nchanells=2`), but it is only used the left-side signal.\n",
    "- It is utilized 16 bits (2 bytes) per sample to encode the audio. The native data type of this data is `int16`, which is capable of storing a [range from](https://www.mathworks.com/help/matlab/ref/audioread.html) `-32768` up to `+32767`.\n",
    "- The data type is converted to `float` because of the numeric precision and because the floating point in `Python` [is interpreted as](https://docs.python.org/3/library/stdtypes.html#numeric-types-int-float-complex) `double` in `C`, which is convenient.\n",
    "- The float-converted raw data is then normalized by the maximum value reachable of the `int16` format, that is, `32768`. The resulting signal is the same achieved by the `audioread()` command of `matlab`.\n",
    "- The original sampling rate is $44.1\\;kHz$. But each recording is downsampled into two different signals, with a sampling rate of $F_s = 22.05\\;kHz$.\n",
    "- The audio dataset comprises five classes (the speeches \"avançar\", \"recuar\", \"parar\", \"direita\", and \"esquerda\"), each with 10 recordings, totalizing 50 files. With the downsampling, we have 20 recordings by class. Considering that the `.wav` file is stereo, that is, `nchannel=2`, the number of audio recordings by class is increased to 40. From each of these recordings, it is extracted a discrete-time signal, which is converted to a $N_s$-dimensional vector, being $N_s$ the number of samples of this signal.\n",
    "\n",
    "## Some notes about the LPC (linear predictive coding) and the Yule-Walker algorithms\n",
    "\n",
    "- The AR(p) model is implemented for `p=10`, `p=15`, and `p=20`.\n",
    "- A single recording is divided into 31 frames without overlapping. The number of samples per frames, $N_f$, and the number of samples between each frame, $N_{gap}$, are given by\n",
    "    $$ N_f = \\frac{T_{sig}T_{f_{min}} F_s}{T_{min}} $$\n",
    "    and\n",
    "    $$ N_{gap} = \\frac{T_{sig} F_s-31N_f}{30},$$\n",
    "    where $T_{sig}$ is the signal duration, $T_{min}$ is the minimum signal duration of the dataset, and $T_{f_{min}} \\triangleq 15\\;ms $ is the minimum frame duration. All these variables are defined in seconds.\n",
    "- The Yule-Walker equation is applied to each of the 31 frames produced from a single audio recording. The final vector is achieved by concatenating all the $31$ coefficients obtained by the Yule-Walker equation, it is given by\n",
    "$$\\mathbf{a}_{p} = \\begin{bmatrix}\n",
    "\\mathbf{a}_{p,1}^\\mathsf{T} & \\mathbf{a}_{p,2}^\\mathsf{T} & \\cdots & \\mathbf{a}_{p,31}^\\mathsf{T}\n",
    "\\end{bmatrix}^\\mathsf{T} \\in \\mathbb{R}^{31p},\n",
    "$$\n",
    "where $\\mathbf{a}_{p,i} \\in \\mathbb{R}^p$ is the coefficient vector obtained form the $i$-th frame. This procedure is repeated for each of the four signals (channel a and b, samples even and odd) from a single audio recording, for each audio recording.\n",
    "- For sake of clarity, it  is chosen the normalized (by its l2 norm) version of the autocorrelation function. It makes $r(\\tau)$ invariant to the signal energy of the frame.\n",
    "- It is chosen a split for 50%-50% for train and train dataset, as done in the article.\n",
    "\n",
    "---\n",
    "\n",
    "> 1. Carregar os diversos arquivos de áudio e realizar a subamostragem dos sinais de cada canal a fim\n",
    "de gerar a base de dados de treino e teste.\n",
    "\n",
    "### Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reset -f\n",
    "from numpy import inf, empty, concatenate, arange, inner, array, logical_and, where, identity, logspace\n",
    "from numpy.fft import fft\n",
    "from statsmodels.regression.linear_model import yule_walker\n",
    "from scipy.io import wavfile\n",
    "from scipy.linalg import toeplitz\n",
    "from scipy.signal import periodogram\n",
    "from math import floor\n",
    "from numpy.linalg import norm, cond, matrix_rank as rank, inv\n",
    "from warnings import warn\n",
    "from os import listdir\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# number of files per commands\n",
    "n_files = 10\n",
    "# AR(p) model order -> p = 10, 15, 20\n",
    "all_p = range(10,21,5)\n",
    "# all commands\n",
    "all_commands = ('avancar', 'esquerda', 'direita', 'parar', 'recuar')\n",
    "# a dictionary that gather all signals and the coefficient estimation for each audio file\n",
    "all_data = {f'{command}{file_number}_p{p}_s{signal}': {'signal': empty(0), 'all_a': empty(0), 'all_a_hat': empty(0)} for p in all_p for command in all_commands for file_number in range(1,n_files+1) for signal in ('0a', '0b', '1a', '1b')}\n",
    "\n",
    "def get_T_min(root_dir):\n",
    "    T_min = inf\n",
    "    for file_name in listdir(root_dir):\n",
    "        F_s, s_n = wavfile.read(root_dir+file_name)\n",
    "        # signal duration\n",
    "        T_sig = s_n[:,0].size * (1/F_s)\n",
    "        if T_sig < T_min:\n",
    "            T_min = T_sig\n",
    "    return T_min\n",
    "\n",
    "# minimum audio duration of the dataset\n",
    "T_min = get_T_min('./Audio_files_TCC_Jefferson/')\n",
    "# minimum frame duration, 15ms (user-defined)\n",
    "T_f_min = 15e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LPC and Yule-Walker algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reset_selective -f all_a\n",
    "for p in all_p:\n",
    "    for command in all_commands:\n",
    "        for file_number in range(1,n_files+1):\n",
    "            file_name = f'./Audio_files_TCC_Jefferson/comando_{command}_{file_number:0>2d}.wav'\n",
    "            # input audio vector, s_n -> [s[0], s[1], ..., s[N_s-1]]\n",
    "            F_s, s_n = wavfile.read(file_name)\n",
    "            # Number of samples\n",
    "            N_s = s_n[:,0].size\n",
    "            # signal duration\n",
    "            T_sig = N_s/F_s\n",
    "            # convert from int16 to float type and normalize it to range from -1 up to 1 (as matlab does)\n",
    "            s_n = s_n.astype(float)/32768\n",
    "            # downsampling: generate s0_n (even samples) and s1_n (odd samples) from s_n\n",
    "            s0_n, s1_n = s_n[range(0,N_s,2),:], s_n[range(1,N_s,2),:]\n",
    "            N_s //= 2\n",
    "            F_s //= 2\n",
    "            # number of samples per frame\n",
    "            N_f = floor(T_sig*T_f_min*F_s/T_min)\n",
    "            # number of samples between each frame (gap)\n",
    "            N_gap = floor((N_s - 31*N_f)/30)\n",
    "            # get channel a and chanell b\n",
    "            s0a_n, s0b_n, s1a_n, s1b_n = s0_n[:,0], s0_n[:,1], s1_n[:,0], s1_n[:,1]\n",
    "\n",
    "            # for each of the 4 signals from a single recording: channel a and b, samples even and odd\n",
    "            for s, sig_id in zip((s0a_n, s0b_n, s1a_n, s1b_n), ('0a', '0b', '1a', '1b')):\n",
    "                # save signal\n",
    "                all_data[f'{command}{file_number}_p{p}_s{sig_id}']['signal'] = s\n",
    "                # set the one-hot-encoding\n",
    "                all_data[f'{command}{file_number}_p{p}_s{sig_id}']['d'] = array([command==c for c in all_commands])\n",
    "                # for each frame\n",
    "                for i, n in enumerate(range(0, N_s+1, N_f)):\n",
    "                    # ensure that it is get only 31 frames\n",
    "                    if i == 31:\n",
    "                        break\n",
    "                    # s_n0 -> [s[n0], s[n0+1], ..., s[n0+N_f-1]], being n0\\in\\mathbb{N}\n",
    "                    s_n0 = s[n+i*N_gap:n+i*N_gap+N_f]\n",
    "                    # compute the autocorrelation function (normalized version), r_k -> r[k] -> [r[0], r[1], ..., r[p]]\n",
    "                    r_k = empty(p+1)\n",
    "                    for k in range(p+1):\n",
    "                        r_k[k] = inner(s_n0[:N_f-k], s_n0[k:N_f])/norm(s_n0)\n",
    "                    # autocorrelation matrix\n",
    "                    # r_k[:p] -> [r[0], r[1], ..., r[p-1]]\n",
    "                    R = toeplitz(r_k[:p])\n",
    "                    # autocorrelation vector\n",
    "                    # r -> [r[1], r[2], ..., r[p]]\n",
    "                    r = r_k[1:]\n",
    "                    if rank(R) == R.shape[0]:\n",
    "                        # Yule-Walker equation\n",
    "                        a = inv(R) @ r\n",
    "                        # built-in function for comparison purpose\n",
    "                        a_hat, _ = yule_walker(s_n0, order=p, method='mle')\n",
    "                        # save a\n",
    "                        all_data[f'{command}{file_number}_p{p}_s{sig_id}']['all_a'] = concatenate((all_data[f'{command}{file_number}_p{p}_s{sig_id}']['all_a'], a))\n",
    "                        # save a_hat\n",
    "                        all_data[f'{command}{file_number}_p{p}_s{sig_id}']['all_a_hat'] = concatenate((all_data[f'{command}{file_number}_p{p}_s{sig_id}']['all_a_hat'], a_hat))\n",
    "                    else:\n",
    "                        warn(f'The autocorrelation matrix of the audio {file_name.split(\"/\")[0]} is rank-deficient, skip over to the next audio recording.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine each voice signal using the PSD segmentation method estimated by the periodogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my periodogram estimate function - FROM SCRATCH\n",
    "def my_periodogram(g, Fs):\n",
    "    N = g.size\n",
    "    # Nyquist theorem\n",
    "    W = Fs/2\n",
    "    # FFT\n",
    "    G = fft(g)\n",
    "    # two-sided periodogram estimate\n",
    "    p = abs(G)**2 / (N*Fs)\n",
    "    # transform it into one-sided\n",
    "    p = p[0:1+N//2]\n",
    "    p[1:-1] *= 2\n",
    "    # all frequencies\n",
    "    all_f = arange(0,W+Fs/N,Fs/N)[:p.size]\n",
    "    \n",
    "    return p, all_f\n",
    "\n",
    "# lower and upper bound\n",
    "all_lb = (0, 100, 200, 360, 500, 700, 850, 1000, 1200, 1320, 1500, 2200, 2700)\n",
    "all_ub = all_lb[1:] + (3700,)\n",
    "\n",
    "for key, data in all_data.items():\n",
    "    # As the 4 signals are identical for p \\in {10, 15, 20}, it is not necessary to compute x for different values of p\n",
    "    if 'p10' not in key:\n",
    "        continue\n",
    "    s = data['signal']\n",
    "    p, all_f = my_periodogram(s, F_s)\n",
    "    _, p_builtin = periodogram(s, fs=F_s)\n",
    "    # for each lower and upper bound interval, get the maximum value\n",
    "    psd_sampled = array([max(p[logical_and(all_f>=lb, all_f<=ub)]) for lb, ub in zip(all_lb, all_ub)])\n",
    "    # save the attribute vector and all PSD\n",
    "    all_data[key]['psd sampled'] = psd_sampled\n",
    "    # save PSD from my periodogram estimate\n",
    "    all_data[key]['psd'] = p\n",
    "    # save PSD from my periodogram estimate - built-in function\n",
    "    all_data[key]['psd built-in'] = p_builtin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Least-Squares (LS) classifier + LPC\n",
    "\n",
    "As commented in the article, it is chosen `p=10`, using the Ocham's razor principle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attribute vector size for LPC algorithm -> 31 frames x 10 attributes per frame\n",
    "P_LPC = 310\n",
    "# create a dictionary only with the train dataset\n",
    "iterator = itertools.product(all_commands, range(1,6))\n",
    "all_train_key = [command+str(num) for command, num in iterator]\n",
    "all_train_data = {key:value for key, value in all_data.items() if key.split('_')[0] in all_train_key and 'p10' in key}\n",
    "\n",
    "# attribute matrix\n",
    "X = empty((P_LPC, 0))\n",
    "# one-hot-encoding matrix\n",
    "D = empty((5, 0))\n",
    "for data in all_train_data.values():\n",
    "    X = concatenate((X, data['all_a'][:,None]), axis=1)\n",
    "    D = concatenate((D, data['d'][:,None]), axis=1)\n",
    "# regularized estimation of the linear transformation matrix of the attribute vectors\n",
    "for lambda_ in logspace(0,10,base=2):\n",
    "    W_hat = D @ X.T @ inv(X @ X.T + (lambda_-1)*identity(X.shape[0]))\n",
    "    # if W_hat is ill-conditioned, regularize it\n",
    "    if cond(X @ X.T + (lambda_-1)*identity(X.shape[0])) <= 5e3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the LS classifier to the LPC test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dictionary only with the test dataset\n",
    "iterator = itertools.product(all_commands, range(5,11))\n",
    "all_test_key = [command+str(num) for command, num in iterator]\n",
    "all_test_data = {key:value for key, value in all_data.items() if key.split('_')[0] in all_test_key and 'p10' in key}\n",
    "\n",
    "n_errors = 0\n",
    "\n",
    "for data in all_test_data.values():\n",
    "    x = data['all_a'][:,None]\n",
    "    d = data['d']\n",
    "    d_hat = W_hat @ x\n",
    "    if where(d == max(d))[0] != where(d_hat == max(d_hat))[0]:\n",
    "        n_errors += 1\n",
    "\n",
    "n_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LS classifier + PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'psd sampled'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/tapyu/git/UFC_courses/identificação_de_sistemas/Projeto 1 - Extração de Atributos de Sinais de Voz para Classificação de Padrões/TC1.ipynb Cell 12'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tapyu/git/UFC_courses/identifica%C3%A7%C3%A3o_de_sistemas/Projeto%201%20-%20Extra%C3%A7%C3%A3o%20de%20Atributos%20de%20Sinais%20de%20Voz%20para%20Classifica%C3%A7%C3%A3o%20de%20Padr%C3%B5es/TC1.ipynb#ch0000015?line=5'>6</a>\u001b[0m D \u001b[39m=\u001b[39m empty((\u001b[39m5\u001b[39m, \u001b[39m0\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tapyu/git/UFC_courses/identifica%C3%A7%C3%A3o_de_sistemas/Projeto%201%20-%20Extra%C3%A7%C3%A3o%20de%20Atributos%20de%20Sinais%20de%20Voz%20para%20Classifica%C3%A7%C3%A3o%20de%20Padr%C3%B5es/TC1.ipynb#ch0000015?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m all_train_data\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tapyu/git/UFC_courses/identifica%C3%A7%C3%A3o_de_sistemas/Projeto%201%20-%20Extra%C3%A7%C3%A3o%20de%20Atributos%20de%20Sinais%20de%20Voz%20para%20Classifica%C3%A7%C3%A3o%20de%20Padr%C3%B5es/TC1.ipynb#ch0000015?line=7'>8</a>\u001b[0m     X \u001b[39m=\u001b[39m concatenate((X, data[\u001b[39m'\u001b[39;49m\u001b[39mpsd sampled\u001b[39;49m\u001b[39m'\u001b[39;49m][:,\u001b[39mNone\u001b[39;00m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tapyu/git/UFC_courses/identifica%C3%A7%C3%A3o_de_sistemas/Projeto%201%20-%20Extra%C3%A7%C3%A3o%20de%20Atributos%20de%20Sinais%20de%20Voz%20para%20Classifica%C3%A7%C3%A3o%20de%20Padr%C3%B5es/TC1.ipynb#ch0000015?line=8'>9</a>\u001b[0m     D \u001b[39m=\u001b[39m concatenate((D, data[\u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m'\u001b[39m][:,\u001b[39mNone\u001b[39;00m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tapyu/git/UFC_courses/identifica%C3%A7%C3%A3o_de_sistemas/Projeto%201%20-%20Extra%C3%A7%C3%A3o%20de%20Atributos%20de%20Sinais%20de%20Voz%20para%20Classifica%C3%A7%C3%A3o%20de%20Padr%C3%B5es/TC1.ipynb#ch0000015?line=10'>11</a>\u001b[0m \u001b[39m# regularized estimation of the linear transformation matrix of the attribute vectors\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'psd sampled'"
     ]
    }
   ],
   "source": [
    "# attribute vector size -> 13 frames of the PSD\n",
    "P_PSD = 13\n",
    "# attribute matrix\n",
    "X = empty((P_PSD, 0))\n",
    "# one-hot-encoding matrix\n",
    "D = empty((5, 0))\n",
    "for data in all_train_data.values():\n",
    "    X = concatenate((X, data['psd sampled'][:,None]), axis=1)\n",
    "    D = concatenate((D, data['d'][:,None]), axis=1)\n",
    "    \n",
    "# regularized estimation of the linear transformation matrix of the attribute vectors\n",
    "for lambda_ in logspace(0,10,base=2):\n",
    "    W_hat = D @ X.T @ inv(X @ X.T + (lambda_-1)*identity(X.shape[0]))\n",
    "    # if W_hat is ill-conditioned, regularize it\n",
    "    if cond(X @ X.T + (lambda_-1)*identity(X.shape[0])) <= 5e3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the LS classifier to the PSD test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_errors = 0\n",
    "\n",
    "for data in all_test_data.values():\n",
    "    x = data['psd sampled'][:,None]\n",
    "    d = data['d']\n",
    "    d_hat = W_hat @ x\n",
    "    if where(d == max(d))[0] != where(d_hat == max(d_hat))[0]:\n",
    "        n_errors += 1\n",
    "\n",
    "n_errors"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d7aca15fd486567410f0b50ac6cefe479aeb320e6005d7aafae29a8f55f6329"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('identificação-de-sistemas-sFYUFp5A-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
